[{"title":"ES基本使用及问题处理01","url":"/2025/09/08/ES%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E5%8F%8A%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%8601/","content":"业务进行压测，后面造成ES数据满了，有90%告警。针对该情况需要清理ES，数据可以正常写入。\n清理数据curl 10.25.83.242:9200/_cluster/health?pretty #查看监控curl 10.25.83.242:9200/_cat/indices?v |grep gb #查看数据curl -XDELETE 10.25.83.242:9200/xxxxx #删除数据\n\n但是发现清理数据完候，ES只读，数据写不进数据\n原因分析只读标记未手动清除：这是最常见的原因。Elasticsearch 不会自动将索引的 read_only_allow_delete 属性改回 false\n磁盘空间未真正有效释放：\n\n延迟释放：使用 DELETE 操作或 _delete_by_query 进行数据删除时，Elasticsearch 采用的是“标记删除”而非立即从磁盘擦除。真正的空间释放需要等待 Segment Merge 过程完成。\n\n清理量不足：可能删除的数据量不足以让所有磁盘的使用率都降到水位线（特别是 flood_stage, 例如95%）以下。Elasticsearch 会检查所有节点磁盘空间。\n\n\n水位线检查有延迟或缓存：Elasticsearch 检测磁盘空间和使用率变化可能存在短暂延迟。\n分片分配问题：即使空间足够，如果之前因磁盘空间不足导致分片未分配，空间释放后可能需要手动恢复分片分配或等待集群自动重新平衡。\n📖 总结：问题解决步骤\n\n\n步骤\n操作\n命令&#x2F;操作示例\n说明\n\n\n\n1️⃣\n确认磁盘空间\nGET /_cat/allocation?v\n查看所有节点的磁盘使用率，确保均低于洪水水位线（如95%）\n\n\n2️⃣\n检查索引设置\nGET your_index_name/_settings\n确认 index.blocks.read_only_allow_delete 是否为 true\n\n\n3️⃣\n手动解除只读\ncurl  -H “Content-Type:application&#x2F;json” -XPUT 10.25.83.242:9200&#x2F;_cluster&#x2F;settings -d ‘{   “persistent”: {     “cluster.blocks.read_only_allow_delete”: false   } }’\n核心步骤：手动清除索引的只读标记\n\n\n4️⃣\nxxx\ncurl -H “Content-Type: application&#x2F;json”  -XPUT ‘10.25.83.242:9200&#x2F;_all&#x2F;_settings’ -d ‘{“index”:{“blocks”:{“read_only_allow_delete”:null}}}’\n\n\n\n5️⃣\n验证写入\nPOST your_index_name/_doc &#123; &quot;test&quot;: &quot;data&quot; &#125;\n尝试写入一条测试数据，验证是否成功\n\n\n","categories":["中间件"],"tags":["ES"]},{"title":"Chronyd 安装与配置排错指南","url":"/2025/09/07/Chronyd-%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%E6%8E%92%E9%94%99%E6%8C%87%E5%8D%97/","content":"Chronyd 安装与配置排错指南目录\nChronyd 安装与配置\n端口检查与防火墙配置\n时间同步状态诊断\n硬件时钟问题解决方案\n时区配置\n\n\n1. Chronyd 安装与配置安装 Chronydyum install chronyd -y\n\n检查时间同步源chronyc sources -v\n\n验证 Chronyd 服务状态netstat -nlap | grep 123  # 查看 UDP 端口，确认 chronyd 已启动\n\n\n2. 端口检查与防火墙配置检查 UDP 端口连通性# 方案1：使用 nmapnmap -sU 192.168.70.2 -p 123 -Pn# 方案2：使用 netcatnc -uvz 192.168.70.2 123\n\n端口状态说明\n正常状态: 123/udp open ntp\n异常状态: 123/udp open|filtered ntp (表示端口不可用)\n\n配置防火墙规则 (UFW 示例)# 查看当前防火墙规则ufw status# 开放 NTP 服务所需端口ufw allow 123/udpufw allow 323/udp\n\n\n3. 时间同步状态诊断异常状态示例# 检查 chrony 服务状态chronyc sources# 异常输出示例：210 Number of sources = 1MS Name/IP address         Stratum Poll Reach LastRx Last sample===============================================================================^? 192.168.70.2                  0   9     0     -     +0ns[   +0ns] +/-    0ns\n\n正常状态示例# 正常输出示例：210 Number of sources = 1MS Name/IP address         Stratum Poll Reach LastRx Last sample===============================================================================^* 192.168.70.2                 10   6    17     1    -12us[+23us] +/- 276us\n\n\n4. 硬件时钟问题解决方案检查当前时间状态date                    # 查看系统时间hwclock --show          # 查看硬件时钟时间\n\n硬件时钟同步操作# 手动设置硬件时钟时间hwclock --set --date &#x27;2018-08-20 14:05:25&#x27;# 同步硬件时钟与系统时间hwclock --hctosys       # 硬件时间同步到系统时间hwclock --systohc       # 系统时间同步到硬件时间# 保存时钟设置clock -w\n\n高级时间管理# 查看时间同步源状态chronyc sourcestats -v# 立即手动同步时间（适用于时间偏差较大时）chronyc -a makestep# 校准时间服务器chronyc tracking\n\n硬件时钟配置# 检查硬件时钟配置timedatectl | grep local# 配置硬件时钟使用本地时区timedatectl set-local-rtc 1# 配置硬件时钟使用 UTC（推荐）timedatectl set-local-rtc 0# 启用 NTP 时间同步timedatectl set-ntp yes\n\n\n5. 时区配置更改时区（例如从 EST 改为 CST）# 备份原时区配置mv /etc/localtime /etc/localtime.bak# 设置上海时区（中国标准时间 CST）ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime\n\n\n故障排除流程图\n时间不同步\n检查 chronyc sources 输出\n验证 NTP 端口连通性\n确认防火墙配置\n\n\n硬件时钟与系统时间不一致\n使用 hwclock --hctosys 或 hwclock --systohc 同步\n检查时区设置是否正确\n\n\n时区错误\n使用 timedatectl 检查当前时区\n重新链接正确的时区文件\n\n\n\n\n常用命令速查表\n\n\n命令\n功能描述\n\n\n\nchronyc sources\n查看时间同步源状态\n\n\nchronyc -a makestep\n立即强制时间同步\n\n\nhwclock --hctosys\n硬件时间同步到系统时间\n\n\nhwclock --systohc\n系统时间同步到硬件时间\n\n\ntimedatectl set-local-rtc 1\n设置硬件时钟使用本地时区\n\n\ntimedatectl set-ntp yes\n启用 NTP 时间同步\n\n\n\n适用系统: CentOS&#x2F;RHEL 7+、Ubuntu 16.04+\n\n提示：生产环境中建议将硬件时钟设置为 UTC (timedatectl set-local-rtc 0)，并确保防火墙正确配置允许 NTP 流量。\n\n","categories":["Linux"],"tags":["chrony"]},{"title":"clickhouse分布式基本使用","url":"/2025/09/08/clickhouse%E5%88%86%E5%B8%83%E5%BC%8F%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","content":"目前clickhouse创建是2副本的,业务页面访问clickhouse 发现两个clickhouse的数据不同步。主要原因是未创建分布式clickhouse实例。目前针对该场景进行记录clickhouse分布式的使用\n容器登陆clickhouse数据库kubectl  exec -it -n sso     clickhouse-sso-test-clickhhouse-0-ss-0  -c clickhouse -- clickhouse-client --user root --password testclickhouse  \n\n或者使用curl连接clickhouse目前业务可依据使用情况，如使用代码进行创建。或者使用可视化视图进行创建。\ncurl -X POST \\  &quot;http://10.25.83.231:8123/?user=root&amp;password=testclickhouse&quot; \\  -d &quot;create database test0814 on cluster defaultCluster;&quot;\n\n创建分布式库#test0813 为数据库名#defaultCluster 的 ClickHouse 集群create database test0813 on cluster defaultCluster;\n\n创建本地表#SHOW CREATE TABLE test0813.local_table 可查看表结构CREATE TABLE test0813.local_table ON CLUSTER defaultCluster(    event_date Date,    user_id UInt64,    action_type Enum(&#x27;login&#x27;, &#x27;logout&#x27;),    device Array(String))ENGINE = ReplicatedMergeTree(&#x27;/clickhouse/tables/&#123;shard&#125;/test0813/local_table&#x27;, &#x27;&#123;replica&#125;&#x27;)PARTITION BY toYYYYMM(event_date)ORDER BY (user_id, action_type)SETTINGS index_granularity = 8192;\n\n说明：\n1.test0813.local_table：创建在 test0813 数据库中的 local_table 表\n2.ON CLUSTER defaultCluster：在名为 defaultCluster 的 ClickHouse 集群所有节点上创建此表\n3.ReplicatedMergeTree这个引擎用于创建具有复制功能的表，确保数据在多个副本之间同步。\n\n\n\n路径组件\n说明\n示例值\n\n\n\n&#x2F;clickhouse&#x2F;tables\n根路径（ClickHouse 约定）\n固定值\n\n\n{shard}\n分片标识符占位符\n01, shard1\n\n\ntest0813\n数据库名称\n您的数据库名\n\n\nlocal_table\n本地表名称\n您的表名\n\n\n4.PARTITION BY toYYYYMM(event_date) – 按月分区\n5.ORDER BY (user_id, action_type) – 主键索引\n6.SETTINGS index_granularity &#x3D; 8192 – 索引粒度\n索引粒度：每 8192 行生成一个索引标记性能影响：值越小 → 索引更精细 → 查询更快 → 索引更大值越大 → 索引更粗糙 → 查询稍慢 → 索引更小默认值：8192（平衡选择）\n\n\n\n创建分布式表CREATE TABLE test0813.user_data ON CLUSTER defaultCluster(    event_date Date,    user_id UInt64,    action_type Enum(&#x27;login&#x27;, &#x27;logout&#x27;),    device Array(String))ENGINE = Distributed(    defaultCluster,   -- 集群名称    &#x27;test0813&#x27;,       -- 数据库名    &#x27;local_table&#x27;,    -- 底层本地表名    rand()            -- 分片键（随机分布）);\n\n插入数据# 通过分布式表插入（自动路由）INSERT INTO test0813.user_data VALUES    (&#x27;2023-08-11&#x27;, 1001, &#x27;login&#x27;, [&#x27;iPhone&#x27;,&#x27;iOS 15&#x27;]),    (&#x27;2023-08-11&#x27;, 1002, &#x27;logout&#x27;, [&#x27;Android&#x27;,&#x27;Chrome&#x27;]),    (&#x27;2023-08-12&#x27;, 1003, &#x27;login&#x27;, [&#x27;Windows&#x27;,&#x27;Firefox&#x27;]);\n\n删除数据#安全删除分布式表，删除分布式表（不影响数据）DROP TABLE IF EXISTS test0813.user_data ON CLUSTER defaultCluster#删除本地表（永久删除数据）DROP TABLE IF EXISTS test0813.local_table ON CLUSTER defaultCluster\n\n其他说明\n创建多张表，是否需要创建多张local_table的表？需要的\n需要关闭一台节点，查看数据是否同步。会同步的\n\n","categories":["中间件"],"tags":["clickhouse"]},{"title":"shell之获取docker容器中的cpu与内存使用率","url":"/2025/09/06/shell%E4%B9%8B%E8%8E%B7%E5%8F%96docker%E5%AE%B9%E5%99%A8%E4%B8%AD%E7%9A%84cpu%E4%B8%8E%E5%86%85%E5%AD%98%E4%BD%BF%E7%94%A8%E7%8E%87/","content":"背景：该项目迁移至新环境需求，将原先TSF集群（节点数：xxx）迁移到新环境中，jmeter 压测一个查询接口的数据。分别在旧环境与新环境的接口数据进行压测，对比监控CPU、内存利用率情况。目前旧环境容器无监控，故思路上使用脚本来获取监控数据，获取的数据用图标展示出来。底层的k8s比较老，暂未支持kubectl  top  容器。\n问题:为什么不进入容器中查看，top查看容器的资源使用量。这是因为我们在容器中运行 top 命令，虽然可以看到容器中每个进程的 CPU 使用率，但是 top 中”%Cpu(s)”那一行中显示的数值，并不是这个容器的 CPU 整体使用率，而是容器宿主机的 CPU 使用率。\n#!/bin/bashHOST_NAME=$(hostname -I | awk &#x27;&#123;print $2&#125;&#x27;)LOG_FILE=&quot;/var/log/docker_stats_20250719_$HOST_NAME.log&quot;DOCKER_NAME=$(docker ps |grep  &quot;tsf_1/element-server&quot; |awk &#x27;&#123;print $1&#125;&#x27;)INTERVAL=5 # 收集间隔（秒）# 创建日志文件并添加表头echo &quot;时间戳,容器ID,CPU使用率%,内存使用率%&quot; &gt; &quot;$LOG_FILE&quot;while true; do    # 获取当前时间戳    TIMESTAMP=$(date &#x27;+%H:%M:%S&#x27;)    # 使用 top 收集 CPU 使用率（提取 %Cpu(s): 后的数值）    CPU_MEM_USAGE=$(docker stats $DOCKER_NAME --no-stream --format &quot;&#123;&#123;.Container&#125;&#125;,\\t&#123;&#123;.CPUPerc&#125;&#125;,\\t\\t&#123;&#123;.MemPerc&#125;&#125;&quot;)    echo $CPU_MEM_USAGE    #top -bn1 | grep &#x27;%Cpu&#x27; | awk &#x27;&#123;print &quot;CPU使用率: &quot; 100 - $8 &quot;%&quot;&#125;&#x27;    echo &quot;$TIMESTAMP， $CPU_MEM_USAGE &quot;  &gt;&gt; &quot;$LOG_FILE&quot;    # 等待下一次收集    sleep $INTERVALdone\n\n最后制作出来的图标如下：\n\n关于得到的数据的疑问的记录\n从图标上可以得出有的CPU超100%了，原因是什么呢？正常来看这个CPU应该是多少的\n\n绝对资源消耗量\n\n307% = 3.07 个逻辑 CPU 核心满载#表示容器当前每秒消耗 3.07 核心秒的计算资#相当于： 3 个核心 100% 满载 + 第 4 个核心 7% 负载 或 4 个核心平均 76.75% 负载\n\n\ndocker stats 中查看 以下参数 docker 有关cpu配置的区别：\n\n[root@x.x.x.x ~]# docker inspect 1f77b1a2b98e |grep -i cpu            &quot;CpuShares&quot;: 4096, #cpu request的值            &quot;NanoCpus&quot;: 0,            &quot;CpuPeriod&quot;: 100000,            &quot;CpuQuota&quot;: 400000, #cpu limit的值            &quot;CpuRealtimePeriod&quot;: 0,            &quot;CpuRealtimeRuntime&quot;: 0,            &quot;CpusetCpus&quot;: &quot;9-12&quot;, #运行在哪个cpu上            &quot;CpusetMems&quot;: &quot;&quot;,            &quot;CpuCount&quot;: 0,            &quot;CpuPercent&quot;: 0,\n\n","categories":["工具"],"tags":["docker","shell"]},{"title":"容器化之细节问题处理篇01","url":"/2025/09/06/%E5%AE%B9%E5%99%A8%E5%8C%96%E4%B9%8B%E7%BB%86%E8%8A%82%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86%E7%AF%8701/","content":"问题现象：容器1，我们可以看到：\nPID 1： Java进程\n\n容器2，我们可以看到：\nPID 1: /sbin/tini -- ./entrypoint.shPID 6: Java进程\n\n关键区别在于：第二个容器的entrypoint.sh脚本可能是通过tini启动的。而第一个容器可能是直接以entrypoint.sh作为入口点，没有使用tini，所以在exec执行后Java进程替换了shell进程成为PID 1。\ndocker ps -a --no-trunc |grep xxx #查看docker 启动命令进行确认,使用docker run时指定了init程序为tini。\n\n这是因为在Dockerfile中可能使用了类似以下的指令：ENTRYPOINT [“&#x2F;sbin&#x2F;tini”, “–”, “.&#x2F;entrypoint.sh”]\ncat entrypoint.sh #!/bin/sh -eecho &quot;The application will start in $&#123;APP_SLEEP&#125;s...&quot; &amp;&amp; sleep $&#123;APP_SLEEP&#125;exec /usr/bin/java -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap $&#123;JAVA_OPTS&#125; \\     \t-Djava.security.egd=file:/dev/./urandom \\      -Dtsf.swagger.enabled=false \\     \t-jar /root/app/element-server2*.jar $&#123;RUN_ARGS&#125; &quot;$@&quot;\n\n\ntini 的作用\n\n1.tini 是一个轻量级的初始化系统，专门为容器设计\n\n1.可以确保它能正确接收 Docker 停止命令发送的信号\n\n2.它作为 PID 1 运行，负责管理子进程（这里是 entrypoint.sh）\n\n3.entrypoint.sh 脚本仍然使用 exec 启动 Java，但此时：\n  exec 替换的是 entrypoint.sh 进程（PID 6），而不是 PID 1\n  tini 保持为 PID 1，Java 进程作为其子进程运行\n\n\n两种方式的比较\n\n\n特性\n直接 exec (无 tini)\n使用 tini\n\n\n\nPID 1 进程\nJava 应用\ntini\n\n\n信号处理\n应用需自行处理\ntini 协助处理\n\n\n僵尸进程回收\n可能存在问题\n自动回收\n\n\n进程树结构\n更简单\n稍复杂但更健壮\n\n\n","categories":["容器化"],"tags":["docker"]},{"title":"容器化之细节问题处理篇02","url":"/2025/09/07/%E5%AE%B9%E5%99%A8%E5%8C%96%E4%B9%8B%E7%BB%86%E8%8A%82%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86%E7%AF%8702/","content":"背景：项目上需要将服务的日志都显示出来，对于有两类日志文件不能正常显示日志出来的。情况1：业务服务中未将服务日志打印在日志文件中，查看容器所在节点有查看到0.log 类似的日志，需要确认该日志是否就是服务的运行日志，目前针对该种日志产生的过程流程梳理\n容器的标准输出&#x2F;var&#x2F;log&#x2F;pods&#x2F;…&#x2F;0.log 文件的产生过程是：\nApp 输出 -&gt; stdout/stderr运行时捕获 -&gt; 容器运行时（Docker/containerd）将其写入自己的 JSON 日志文件中。kubelet 组织 -&gt; kubelet 创建清晰的目录结构，并通过符号链接指向容器运行时的原始日志文件。对外接口 -&gt; 这个路径成为了该容器日志在节点上的标准化、稳定的访问点。无论是系统管理员、日志采集代理（如 Filebeat、Fluentd），还是 kubectl logs 命令，最终都是通过读取这个路径下的文件来获取日志内容的。kubectl logs 命令的工作原理就是：apiserver -&gt; 对应节点的 kubelet -&gt; kubelet 读取 /var/log/pods/.../0.log 文件中的内容，解析 JSON 格式，只提取 log 字段的纯文本信息，然后流式传输回给请求者。\n\n关系与区别\n\n\n特性\n/var/log/pods/.../0.log (源文件:类比数据库原始表)\nkubectl logs (解析后的视图:类比执行SQL后的视图)\n\n\n\n内容\n原始、完整的数据\n解析后、筛选过的内容\n\n\n格式\nJSON格式，每行一条完整的JSON记录\n纯文本，只提取了JSON中的日志消息本身\n\n\n元数据\n包含丰富元数据，如完整的时间戳、流类型（stdout&#x2F;stderr）\n不包含JSON中的元数据，但可能会格式化输出时间戳（需加 --timestamps）\n\n\n访问方式\n直接登录到节点服务器查看文件\n通过Kubernetes API Server远程查询\n\n\n主要使用者\n系统、日志采集代理（如Fluentd, Filebeat）\n开发者、运维人员\n\n\n情况2：业务服务中日志打印显示是需要日志文件的挂载，目前挂载文件日志显示如下。mountPath: &#x2F;root&#x2F;app (服务启动jar在改目录) 则会挂载失败。原因:共享卷会把其他镜像内置的文件清空的。所以使用该种方式最好统一放在日志文件中，如下是该种情况的最佳实践\n        volumeMounts:        - mountPath: /root/app/log          name: test-log-l9ynkeyd-1...      volumes:      - emptyDir: &#123;&#125;        name: test-log-l9ynkeyd-1\n\n存储类型总结对比表格:\n\n\n存储类型\n概念理解\n生命周期\n\n\n\nemptyDir\n临时存储，会在节点创建空目录\n生命周期&#x3D;pod\n\n\nhostPath\n允许挂载宿主机文件到pod\n生命周期&#x3D;宿主机\n\n\nPVC&#x2F;PV\nKubernetes中持久存储的抽象，用户通过声明PVC请求存储资源，而PVC会绑定到满足其需求的PV\n持久存储\n\n\nStorageClass\n多个StorageClass，用户在创建PVC时指定StorageClass名称，以动态申请存储资源。\n持久存储\n\n\n详细场景说明\n","categories":["容器化"],"tags":["docker","存储"]},{"title":"容器化之细节问题处理篇03","url":"/2025/09/11/%E5%AE%B9%E5%99%A8%E5%8C%96%E4%B9%8B%E7%BB%86%E8%8A%82%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86%E7%AF%8703/","content":"Warning  SystemLoadIsFull  56s (x1313 over 2d2h)  custom-plugin-monitor  (combined from similar events): system load/cores percent exceeded threshold 80%, load/cores percent: 118.09%, load15m: 37.79\n\n告警信息解读触发时长： 56s (x1313 over 2d2h)\n\n**56s**： 本次告警持续了56秒（可能刚刚恢复或正在触发）。\n**x1313 over 2d2h**： 它在过去 2天2小时 内已经触发了 1313次。\n\n监控源： custom-plugin-monitor\n告警原因： system load/cores percent exceeded threshold 80%\n\n告警规则是：（负载平均值 &#x2F; CPU核心数）的百分比 &gt; 80% 即触发告警。\n\n具体数值： load/cores percent: 118.09%, load15m: 37.79\n\n**load15m: 37.79**： 过去15分钟的系统平均负载是 37.79。\n**load/cores percent: 118.09%**： **(37.79 &#x2F; 核心数) * 100% &#x3D; 118.09%**。\n\n核心问题计算与确认根据告警提供的百分比，我们可以反推出监控系统认为的CPU核心数：\n核心数 = load15m / (load/cores percent) = 37.79 / 1.1809 ≈ 32 个核心\n\n结论：\n\n告警阈值是 80%，即当负载超过 0.8 * 32 = 25.6 时就会告警。\n而当前负载达到了 37.79，是阈值的 ~1.5倍，超出程度很高。\n负载&#x2F;核心比达到了 **118%**，意味着平均有 37.79个进程 在竞争 32个CPU核心，平均至少有 5-6个进程 在永远地排队等待，系统长期处于严重过载状态。\n\nTOP的信息解读\n负载是： 26.05, 37.57, 43.67 (1分钟，5分钟，15分钟平均负载)\n像只有32个收银台，却平均有43.67个顾客要结账。这意味着平均有超过11个进程一直在排队等待获得CPU资源（或者是在等待磁盘I&#x2F;O完成）。\n\n空闲率\nCpu0: us 13.9%, sy 9.9%, id 73.5% -&gt; 空闲率73.5%，还算空闲。Cpu16: us 17.2%, sy 10.5%, id 69.9% -&gt; 空闲率69.9%Cpu19: us 19.3%, sy 9.8%, id 67.6% -&gt; 空闲率67.6%Cpu20: us 22.8%, sy 10.4%, id 64.1% -&gt; 空闲率64.1%\n有一些核心的idle（空闲）百分比低于70%，而有些核心（如Cpu11, Cpu12, Cpu13, Cpu14, Cpu15）的空闲率在97%以上。这表明负载并不均衡，可能有一些进程绑定到了特定的CPU核心上，或者某些进程在多个核心上运行但分布不均。\n\n高CPU进程: \nPID 3995612 (java): 占用396.1% CPU (约4个核心的满载)\nPID 1677 (dockerd): 占用41.1% CPU\nPID 3995595 (containerd-shim): 占用21.1% CPU\n\n为什么您的系统负载高，但CPU使用率看起来不算极高？\n1.%wa (I&#x2F;O Wait),几乎所有核心的 %wa 都是 0.0。所有高纯粹的CPU计算资源竞争。\n2.这些进程对CPU的总需求（43.67）超过了系统能同时提供的数量（32），所以它们必须排队。这个排队队伍的平均长度就是负载平均值(43.67)。【理解等于PID的所再占的CPU值大于32】\n\n\n"},{"title":"我的第一篇博客","url":"/2025/09/06/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/","content":"欢迎来到我的博客！\n一下是开启我博客的开始之路\n一、创建于 Hexo + GitHub Pages 的搭建个人博客网站的详细教程\n二、更新博客的源码（Markdown文章、主题、配置等）通常存放在一个GitHub仓库中，而通过Hexo生成的静态网站文件则部署到GitHub Pages（通常是 username.github.io 仓库的特定分支，如 gh-pages 或 main）。\n\n📝 手动更新发布流程（本地完成）这种方式需要在计算机上配置好Hexo环境（Node.js, Git, Hexo等）。\n\n新建博文：使用命令创建新的Markdown文件。\nhexo new &quot;文章题目&quot;  # 或 hexo n &quot;文章题目&quot;\n\n\n\n这会在 source/_posts 目录下生成一个 文章题目.md 文件。\n\n编写&#x2F;更新内容：用文本编辑器（如VS Code）编辑生成的Markdown文件。\n\n生成静态文件：在Hexo站点根目录下执行命令，将Markdown转换为HTML等静态资源。\nhexo generate  # 或 hexo g\n\n\n\n如果需要先清除缓存和旧文件，可以使用：\nhexo clean &amp;&amp; hexo generate```:cite[1]\n\n\n\n本地预览（可选但推荐）：在部署前，最好先在本地启动服务器检查效果。\nhexo server  # 或 hexo s \n\n\n\n然后在浏览器中访问 http://localhost:4000 查看。\n\n部署到GitHub Pages：将生成好的静态文件（通常在 public 目录）推送至GitHub仓库的特定分支。\nhexo deploy  # 或 hexo d\n\n\n\n常用的便捷组合命令是：\nhexo clean &amp;&amp; hexo g &amp;&amp; hexo d为了能使用 `hexo deploy` 命令，您需要：在Hexo的**站点配置文件** (`_config.yml`) 中正确设置部署信息:    deploy:      type: git      repo: &lt;您的GitHub仓库SSH或HTTPS地址&gt;  # 例如 git@github.com:username/username.github.io.git      branch: &lt;部署分支，如gh-pages或main&gt; \n\n\n确保已安装 hexo-deployer-git 插件:cite[4]:cite[6]：npm install hexo-deployer-git --save\n\n\n推送源码（重要）：hexo deploy 通常只部署生成的静态文件。别忘了将您的博客源码（Markdown文章、主题、配置文件等）也推送到GitHub仓库的另一个分支（例如 source 或 hexo-source），以便备份和在多设备间同步。\n\n\n三、next 客制化页面3.1 字体设置\nfont:  #enable: false  enable: true  # Uri of fonts host, e.g. https://fonts.googleapis.com (Default).  host:  # Font options:  # `external: true` will load this font family from `host` above.  # `family: Times New Roman`. Without any quotes.  # `size: x.x`. Use `em` as unit. Default: 1 (16px)  # Global font settings used for all elements inside &lt;body&gt;.  global:    external: true    family: sans-serif    size: 0.725em  # Font settings for site title (.site-title).  title:    external: true    family:    size:  # Font settings for headlines (&lt;h1&gt; to &lt;h6&gt;).  headings:    external: true    family:    size:  # Font settings for posts (.post-body).  posts:    external: true    family:  # Font settings for &lt;code&gt; and code blocks.  #Roboto  codes:    external: true    family: Fira Code      size: .6em\n\n\n\n3.2 页面字体设置\ncat  themes/next/source/css/_variables/base.styl$font-size-base           = (hexo-config(&#x27;font.enable&#x27;) and hexo-config(&#x27;font.global.size&#x27;) is a &#x27;unit&#x27;) ? unit(hexo-config(&#x27;font.global.size&#x27;), em) : .8125em;//$font-size-base           = .8125em; #统一更改这个大小\n\n3.3 生效并部署\nhexo clean &amp;&amp; hexo g &amp;&amp; hexo d #清除缓存且浏览器也需无痕\n\n3.4 图片问题与设置\n#参数开启+插件安装+用hexo n 文档 去生成文档文件1.将_config.yml 文件中的post_asset_folder 选项设为 true 来打开2.npm install https://github.com/CodeFalling/hexo-asset-image --save\n\n图片设置不生效  标签与分类配置  站点导航\n","categories":["杂篇"]},{"title":"记一次网卡修改及nmcli 使用","url":"/2025/09/06/%E8%AE%B0%E4%B8%80%E6%AC%A1%E7%BD%91%E5%8D%A1%E4%BF%AE%E6%94%B9%E5%8F%8Anmcli-%E4%BD%BF%E7%94%A8/","content":"因需要修改系统参数网卡队列，需要将其机器需要重启。目前发现集群一共100多台机器，重启其中一台master机器后，一直处于notReady状态。原因是因为改机器为双网卡，但是因为网卡名未固定。造成使用为固定的网卡未启动成功。\n解决思路：\n1. 查询100多台机器，/etc/udev/rules.d/70-persistent-net.rules  文件是否都含2条记录2.针对未含的节点操作，添加该条信息，网卡名不可修改，要与原先重启前一致。添加后如下：SUBSYSTEM==&quot;net&quot;, ACTION==&quot;add&quot;, DRIVERS==&quot;?*&quot;, ATTR&#123;address&#125;==&quot;fa:16:3e:98:70:9c&quot;, NAME=&quot;ens3&quot;SUBSYSTEM==&quot;net&quot;, ACTION==&quot;add&quot;, DRIVERS==&quot;?*&quot;, ATTR&#123;address&#125;==&quot;fa:16:3e:f5:e0:f1&quot;, NAME=&quot;ens10&quot; #更新的目标网卡名\n\n拓展：因加的网卡名变更为ens4，造成网卡一直up不起来。\n[root@tcs-10-25-83-48 network-scripts]# nmcli  device conn ens4错误：添加/激活新连接失败：Connection &#x27;ens4&#x27; is not available on device ens4 because device is strictly unmanaged\nnmcli处理过程：nmcli conn show  #查看设备连接情况nmcli conn show &quot;System ens4&quot; #查看网卡设置的详情nmcli conn reload #重启设备nmcli conn up ens4 #连接ens4设置nmcli device set &lt;网卡名称&gt; managed yes #激活网卡nmcli device #查看所有的设备nmcli device conn #将所有设备connnmcli device conn ens4 #网卡连接nmcli  networking #查看网络是由设置\n\n原因：\n[root@X-x.1-x.1-x.1-x.1 network-scripts]# cd /etc/NetworkManager/conf.d/[root@X-x.1-x.1-x.1-x.1 conf.d]# ls99-cloud-init.conf  99-container.conf[root@X-x.1-x.1-x.1-x.1 conf.d]# cat 99-cloud-init.conf # Created by cloud-init on instance boot automatically, do not edit.#[main]dns = none[root@X-x.1-x.1-x.1-x.1 conf.d]# cat 99-container.conf# avoid making container interface down[keyfile]unmanaged-devices=*,except:interface-name:ens10,except:interface-name:ens3 #99-container.conf，因容器平台部署完后，网卡名被获取到了并写入到类似文件中，故不建议修改网卡名称(即NAME=&quot;ens10&quot;)\n\n拓展：记录一次因bond文件配置问题造成的交换机网口具有down的情况，没有nmcli工具需要配置网卡配置bond4，配置信息及其含义，记录正常的配置信息及如何再机器节点上进行排查确认网口bond正常\nifcfg-eth0#IP Config for eth0:DEVICE=&#x27;eth0&#x27;HWADDR=xxONBOOT=&#x27;yes&#x27;USERCTL=noBOOTPROTO=noneSLAVE=yes  # 修正拼写错误MASTER=bond1NM_CONTROLLED=no  # 添加此行\n\nifcfg-eth1#IP Config for eth1:DEVICE=&#x27;eth1&#x27;HWADDR=xxxxxONBOOT=&#x27;yes&#x27;MASTER=bond1SLAVE=yesUSERCTL=no  # 修正为有效值BOOTPROTO=noneNM_CONTROLLED=no  # 添加此行\n\nifcfg-bond1#IP Config for bond1:DEVICE=bond1BOOTPROTO=noneIPADDR=10.215.173.44NETMASK=255.255.255.128GATEWAY=10.215.173.1USERCTL=noONBOOT=&#x27;yes&#x27;TYPE=Bond  # 添加此行BONDING_OPTS=&#x27;miimon=100 mode=4 ad_select=0 lacp_rate=fast updelay=200 xmit_hash_policy=2&#x27;  # 修正参数NM_CONTROLLED=no  # 添加此行#mode=4：表示IEEE 802.3ad动态链路聚合（LACP）#miimon=100：每100毫秒检查一次链路状态#ad_select=1：选择策略，1表示备份模式（但根据bonding文档，ad_select的可能值为0、1、2，1表示“备份”似乎不对，通常ad_select=0（stable）或1（bandwidth）或2（count））注意：ad_select=1实际上表示“带宽”（bandwidth），即选择活动聚合组中带宽最大的端口。#lacp_rate=fast：LACP速率，fast表示每1秒发送一次LACP报文（慢速为30秒）#updelay=0：启用端口的延迟时间（毫秒），但这里设置了两次updelay，第一个为0，第二个为200，这可能会被最后一个值覆盖。实际上，在#BONDING_OPTS中，同一个参数出现多次，通常最后一个生效。所以updelay=200可能会覆盖前面的0。#xmit_hash_policy=2：传输哈希策略，2表示使用层3+4（IP和端口）的哈希。\n\n配置问题分析\n ifcfg-bond1 中的问题\n\n重复的 updelay 参数：updelay=0 和 updelay=200 同时存在,最终生效的是最后一个，即 updelay&#x3D;200。\nad_select&#x3D;1：在某些系统版本中可能不被支持\n\ncheck配置好后的状态\n1.ping 网关 \n2.&#x2F;proc&#x2F;net&#x2F;bonding&#x2F;bond1\nBonding Mode: IEEE 802.3ad Dynamic link aggregation - 正确，模式为4。MII Status: up - 绑定接口状态正常。Slave Interface: eth0 和 eth1 都是 up，速度均为 25000 Mbps，全双工，链路失败计数为0，物理地址正确。Aggregator ID: 两个接口的 Aggregator ID 都是 1 - 这表示两个接口成功聚合到同一个聚合组中。Actor/Partner Churn State: 均为 none，表示没有发生扰动，连接稳定。Partner Mac Address: 两个接口的 Partner Mac Address 都是 00:01:00:01:00:01，这表示它们连接到同一个对端设备（交换机）。\n\n3.使用 ethtool eth0 和 ethtool eth1 再次确认链路状态为 Link detected: yes 且速率为 25000Mb&#x2F;s。\n","categories":["Linux"],"tags":["nmcli"]}]